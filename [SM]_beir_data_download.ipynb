{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "KSP1duKDeaDR",
   "metadata": {
    "id": "KSP1duKDeaDR"
   },
   "outputs": [],
   "source": [
    "# @title Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2c5cc-8fc6-4082-9052-69fa0377d770",
   "metadata": {
    "id": "67b2c5cc-8fc6-4082-9052-69fa0377d770"
   },
   "source": [
    "# Information Retrieval Benchmark Data Download\n",
    "\n",
    "This notebook helps to download data from the [BEIR-cellar](https://github.com/beir-cellar/beir/wiki/Datasets-available)[1] to GCS so that it can be used in benchmarking different information retrieval strategies. The general format of this data is that the queries and documents are jsonl files where each line has the keys `_id`, `title` and `text` (See https://github.com/beir-cellar/beir for more information). A subset of the queries and the corpus should be included in a train.tsv file that has the columns query-id, corpus-id and score. The query-id is a string that matches the _id key from the queries jsonl file, the corpus-id is the same but for the corpus and any score with a value greater than 0 indicates that the document is related to the query. Larger numbers indicate a greater level of relevance. A test.tsv file of the same format should also be provided to assess the performance of the customized model. For a concrete example, data from the [CQADupStack English](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/)[2] dataset is retrieved in this colab.\n",
    "\n",
    "The downloaded data can then be used to test different Information Retrieval methods such as semantic search using customized embeddings.\n",
    "\n",
    "This notebook requires that you have a GCP project and permissions to write objects to GCS. To train the custom embedding model the service agent `service-<project_number>@gcp-sa-aiplatform.iam.gserviceaccount.com` must be able to read and write objects to the GCS buckets. See the documentation for more information about the [Vertex service agents](https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents) and [Cloud Storage permissions](https://cloud.google.com/vertex-ai/docs/general/access-control#storage-roles).\n",
    "\n",
    "####References\n",
    "[1]: Thakur, Nandan, et al. \"BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.\" arXiv preprint arXiv:2104.08663 (2021).\n",
    "\n",
    "[2]: Hoogeveen, Doris, Karin M. Verspoor, and Timothy Baldwin. \"CQADupStack: A benchmark data set for community question-answering research.\" Proceedings of the 20th Australasian document computing symposium. 2015.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Wf6RGGlbkENr",
   "metadata": {
    "id": "Wf6RGGlbkENr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "huggingface-hub 0.18.0 requires fsspec>=2023.5.0, but you have fsspec 2023.4.0 which is incompatible.\n",
      "llama-index 0.8.58 requires fsspec>=2023.5.0, but you have fsspec 2023.4.0 which is incompatible.\n",
      "llama-index 0.8.58 requires urllib3<2, but you have urllib3 2.0.4 which is incompatible.\n",
      "llava 1.1.3 requires scikit-learn==1.2.2, but you have scikit-learn 1.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install dependencies.\n",
    "! pip install -U \"gcsfs==2023.4.0\" \"tqdm==4.65.0\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4xFzXmPbY7FC",
   "metadata": {
    "id": "4xFzXmPbY7FC"
   },
   "source": [
    "**Attention**: you may need to restart runtime so that the right package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "g6KhGedykbOk",
   "metadata": {
    "id": "g6KhGedykbOk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 06:59:11.606461: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-21 06:59:11.667336: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-21 06:59:11.667376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-21 06:59:11.668780: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-21 06:59:11.678420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies.\n",
    "import base64\n",
    "from collections.abc import Sequence\n",
    "import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from typing import Optional, Union\n",
    "import zipfile\n",
    "\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d4c7f-bf73-4698-ad78-43dd287420c5",
   "metadata": {},
   "source": [
    "### Configure all the things here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "p8WCy5dzrIf7",
   "metadata": {
    "id": "p8WCy5dzrIf7"
   },
   "outputs": [],
   "source": [
    "#@markdown Authenticate and define constants to be used across the demo. { display-mode: \"form\" }\n",
    "# Authenticate with GCP to run Vertex jobs and save data.\n",
    "\n",
    "\n",
    "# This will bringup a pop-up window.\n",
    "# from google.colab import auth as google_auth\n",
    "# google_auth.authenticate_user()\n",
    "\n",
    "#@markdown Set your project name and the bucket name. The bucket must either exist or you have permissions to create a bucket. Either way you must have object writer permissions to the bucket.\n",
    "# PROJECT_ID = '' # @param {type:\"string\"}\n",
    "BUCKET_NAME = 'my-project-0004-bucket' # @param {type:\"string\"}\n",
    "#@markdown These parameters help specify the download data and locations and can be left as the defaults.\n",
    "INPUT_DATA_PREFIX = 'download/beir' # @param {type:\"string\"}\n",
    "LOCATION = 'us-central1' # @param {type:\"string\"}\n",
    "# See https://github.com/beir-cellar/beir#beers-available-datasets for datasets.\n",
    "BEIR_DATASET_NAME = 'cqadupstack/english' # @param {type:\"string\"}\n",
    "URL_DATASET_NAME = BEIR_DATASET_NAME.split('/')[0]\n",
    "\n",
    "\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "MODEL_NAME = \"text-bison@001\" # @param {type:\"string\"}\n",
    "IMAGE_MODEL_NAME = \"imagegeneration\" # @param {type:\"string\"}\n",
    "\n",
    "#gs://my-project-0004-bucket/matching_engine/\n",
    "\n",
    "VERTEX_API_PROJECT = PROJECT_ID = \"my-project-0004-346516\" #'your-project' #@param {\"type\": \"string\"}\n",
    "VERTEX_API_LOCATION =REGION= 'us-central1' #@param {\"type\": \"string\"}\n",
    "\n",
    "\n",
    "OUTPUT_SUBFOLDER = \"output\"\n",
    "TEMP_DIR = tempfile.mkdtemp()\n",
    "\n",
    "# See https://github.com/beir-cellar/beir\n",
    "DATA_URL = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{URL_DATASET_NAME}.zip\"\n",
    "\n",
    "\n",
    "MAX_COL_WIDTH = 100\n",
    "pd.set_option('display.max_colwidth', MAX_COL_WIDTH)\n",
    "# Create a global GCS Client.\n",
    "STORAGE_CLIENT = storage.Client(project=PROJECT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Qss39mRbeVwY",
   "metadata": {
    "id": "Qss39mRbeVwY"
   },
   "outputs": [],
   "source": [
    "# @markdown Define helper functions for loading the data. { display-mode: \"form\" }\n",
    "\n",
    "\n",
    "def create_hash_filename(\n",
    "    input_str: str, ext: str = '.zip', max_len: int = 48\n",
    ") -> str:\n",
    "  hash_filename = hashlib.sha256(input_str.encode('utf8')).hexdigest()\n",
    "  return f'{hash_filename[:max_len]}{ext}'\n",
    "\n",
    "\n",
    "# Define helper functions\n",
    "def _upload_to_gcs(\n",
    "    base_path: str, bucket: storage.Bucket, gcs_path: str\n",
    ") -> Sequence[str]:\n",
    "  output_paths = []\n",
    "  for local_path in glob.glob(f'{base_path}/**'):\n",
    "    file_name = os.path.basename(local_path)\n",
    "    if os.path.isdir(local_path):\n",
    "      output_paths.extend(\n",
    "          _upload_to_gcs(local_path, bucket, f'{gcs_path}/{file_name}')\n",
    "      )\n",
    "    else:\n",
    "      remote_path = f'{gcs_path}/{file_name}'\n",
    "      full_gcs_path = f'gs://{bucket.name}/{remote_path}'\n",
    "      print(f'Uploading {local_path} to {full_gcs_path}')\n",
    "      tf.io.gfile.copy(local_path, full_gcs_path)\n",
    "      output_paths.append(full_gcs_path)\n",
    "\n",
    "  return output_paths\n",
    "\n",
    "\n",
    "_DEFAULT_CHUNK_SIZE = 1024 * 1024 * 10\n",
    "\n",
    "\n",
    "def download_file(\n",
    "    download_url: str,\n",
    "    bucket_name: str,\n",
    "    base_gcs_prefix: str,\n",
    "    ignore_cache: bool = False,\n",
    "    chunk_size: int = _DEFAULT_CHUNK_SIZE,\n",
    ") -> str:\n",
    "  session = requests.Session()\n",
    "  response = session.get(download_url, stream=True)\n",
    "  file_name = create_hash_filename(download_url)\n",
    "  zip_file_path = os.path.join(TEMP_DIR, file_name)\n",
    "  if ignore_cache or (not os.path.isfile(zip_file_path)):\n",
    "    os.makedirs(os.path.dirname(zip_file_path), exist_ok=True)\n",
    "    content_length = int(response.headers.get('content-length', 0))\n",
    "    with (\n",
    "        open(zip_file_path, 'wb') as f,\n",
    "        tqdm.tqdm(\n",
    "            total=content_length, leave=False, unit='B', unit_scale=True\n",
    "        ) as pbar,\n",
    "    ):\n",
    "      for data in response.iter_content(chunk_size=chunk_size):\n",
    "        f.write(data)\n",
    "        pbar.update(chunk_size)\n",
    "    print(f'Finished downloading {download_url} to {zip_file_path}')\n",
    "  else:\n",
    "    print(f'Using cached version of {zip_file_path}')\n",
    "\n",
    "  return zip_file_path\n",
    "\n",
    "\n",
    "def read_beir_tsv(file_path: str):\n",
    "  return pd.read_csv(\n",
    "      file_path,\n",
    "      sep='\\t',\n",
    "      dtype={'query-id': str, 'corpus-id': str, 'score': int},\n",
    "  )\n",
    "\n",
    "\n",
    "def write_beir_test(file_path: str, df: pd.DataFrame):\n",
    "  df.to_csv(file_path, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "def generate_train_test_split(\n",
    "    input_path: str, train_split: float = 0.7\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "  full_dataset = read_beir_tsv(input_path)\n",
    "  full_dataset = full_dataset.sort_values('query-id', ignore_index=True)\n",
    "  unique_queries = full_dataset['query-id'].unique()\n",
    "  last_query = unique_queries[int(len(unique_queries) * train_split)]\n",
    "  query_loc = full_dataset['query-id'].searchsorted(last_query, side='right')\n",
    "\n",
    "  train_data = full_dataset.iloc[:query_loc].reset_index(drop=True)\n",
    "  test_data = full_dataset.iloc[query_loc:].reset_index(drop=True)\n",
    "  return train_data, test_data\n",
    "\n",
    "\n",
    "def _normalize_gcs_path(gcs_path: str) -> str:\n",
    "  return gcs_path if gcs_path.startswith('gs://') else f'gs://{gcs_path}'\n",
    "\n",
    "\n",
    "ALL_FILES = (\n",
    "    'corpus.jsonl',\n",
    "    'queries.jsonl',\n",
    "    'qrels/train.tsv',\n",
    "    'qrels/test.tsv',\n",
    ")\n",
    "\n",
    "\n",
    "def files_exist_on_gcs(\n",
    "    bucket_name: str,\n",
    "    base_gcs_prefix: str,\n",
    "    dataset_prefix: Optional[str] = None,\n",
    "    expected_files: Sequence[str] = ALL_FILES,\n",
    ") -> list[str]:\n",
    "  output_paths = []\n",
    "  for f_name in expected_files:\n",
    "    if dataset_prefix:\n",
    "      full_path = os.path.join(\n",
    "          bucket_name, base_gcs_prefix, dataset_prefix, f_name\n",
    "      )\n",
    "    else:\n",
    "      full_path = os.path.join(bucket_name, base_gcs_prefix, f_name)\n",
    "\n",
    "    full_path = _normalize_gcs_path(full_path)\n",
    "    if not tf.io.gfile.exists(full_path):\n",
    "      print(f'{full_path} does not exist')\n",
    "      return []\n",
    "    else:\n",
    "      output_paths.append(full_path)\n",
    "\n",
    "  return output_paths\n",
    "\n",
    "\n",
    "def _get_gcs_bucket(bucket_name: str) -> storage.Bucket:\n",
    "  bucket_name = bucket_name.split('gs://')[-1]\n",
    "\n",
    "  if not bucket_name in {b.name for b in STORAGE_CLIENT.list_buckets()}:\n",
    "    bucket = STORAGE_CLIENT.create_bucket(bucket_name, location=LOCATION)\n",
    "  else:\n",
    "    bucket = STORAGE_CLIENT.bucket(bucket_name)\n",
    "  return bucket\n",
    "\n",
    "\n",
    "def download_to_gcs(\n",
    "    download_url: str,\n",
    "    bucket_name: str,\n",
    "    base_gcs_prefix: str,\n",
    "    ignore_cache: bool = False,\n",
    ") -> dict[str, str]:\n",
    "  if (\n",
    "      uploaded_paths := files_exist_on_gcs(\n",
    "          bucket_name, base_gcs_prefix, BEIR_DATASET_NAME\n",
    "      )\n",
    "  ) and not ignore_cache:\n",
    "    print(f'Using existing files on GCS: {uploaded_paths}')\n",
    "    return uploaded_paths\n",
    "\n",
    "  file_path = download_file(\n",
    "      download_url=download_url,\n",
    "      bucket_name=bucket_name,\n",
    "      base_gcs_prefix=base_gcs_prefix,\n",
    "      ignore_cache=ignore_cache,\n",
    "  )\n",
    "  output_dir = tempfile.mkdtemp()\n",
    "\n",
    "  with zipfile.ZipFile(file_path) as z:\n",
    "    for f_name in z.namelist():\n",
    "      if f_name.startswith(BEIR_DATASET_NAME):\n",
    "        print(f'Extracting {f_name} from {file_path}')\n",
    "        z.extract(f_name, output_dir)\n",
    "\n",
    "  # Since CQADupStack English only has a test.tsv file split it into a train and\n",
    "  # test set.\n",
    "  qrels_dir = os.path.join(output_dir, BEIR_DATASET_NAME, 'qrels')\n",
    "  train_path = os.path.join(qrels_dir, 'train.tsv')\n",
    "  test_path = os.path.join(qrels_dir, 'test.tsv')\n",
    "  if not (os.path.exists(train_path) or os.path.exists(test_path)):\n",
    "    raise ValueError(\n",
    "        f'Either test or train or both files must be present in {qrels_dir}'\n",
    "    )\n",
    "\n",
    "  if not os.path.exists(train_path):\n",
    "    train_data, test_data = generate_train_test_split(test_path)\n",
    "    write_beir_test(train_path, train_data)\n",
    "    write_beir_test(test_path, test_data)\n",
    "  elif not os.path.exists(test_path):\n",
    "    train_data, test_data = generate_train_test_split(train_path)\n",
    "    write_beir_test(train_path, train_data)\n",
    "    write_beir_test(test_path, test_data)\n",
    "\n",
    "  bucket = _get_gcs_bucket(bucket_name)\n",
    "  uploaded_paths = _upload_to_gcs(output_dir, bucket, base_gcs_prefix)\n",
    "  # Clean up after ourselves.\n",
    "  shutil.rmtree(output_dir)\n",
    "  return uploaded_paths\n",
    "\n",
    "\n",
    "def get_path_by_type(all_file_paths: Sequence[str]) -> dict[str, str]:\n",
    "  path_by_type = {}\n",
    "  for f in all_file_paths:\n",
    "    if f.endswith('queries.jsonl'):\n",
    "      path_by_type['queries'] = f\n",
    "    elif f.endswith('corpus.jsonl'):\n",
    "      path_by_type['corpus'] = f\n",
    "    elif f.endswith('train.tsv'):\n",
    "      path_by_type['train'] = f\n",
    "    elif f.endswith('test.tsv'):\n",
    "      path_by_type['test'] = f\n",
    "\n",
    "  return path_by_type\n",
    "\n",
    "\n",
    "def download_from_gcs(blob_uri: str, redownload: bool = False) -> str:\n",
    "  blob_path = blob_uri[5:]\n",
    "  output_path = os.path.join(TEMP_DIR, blob_uri)\n",
    "  if redownload or (not os.path.isfile(output_path)):\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(output_path, 'wb') as f:\n",
    "      STORAGE_CLIENT.download_blob_to_file(blob_uri, file_obj=f)\n",
    "\n",
    "  return output_path\n",
    "\n",
    "\n",
    "def load_jsonl_from_gcs(\n",
    "    blob_uri: str, nrows: Optional[int] = None, redownload: bool = False\n",
    ") -> pd.DataFrame:\n",
    "  output_path = download_from_gcs(blob_uri, redownload=redownload)\n",
    "  return pd.read_json(output_path, lines=True, nrows=nrows, dtype={'_id': str})\n",
    "\n",
    "\n",
    "def load_tsv_from_gcs(\n",
    "    blob_uri: str, nrows: Optional[int] = None, redownload: bool = False\n",
    ") -> pd.DataFrame:\n",
    "  output_path = download_from_gcs(blob_uri, redownload=redownload)\n",
    "  return pd.read_csv(output_path, sep='\\t', dtype={'_id': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae340cb-0583-4e7e-a562-6817ee4d7f6d",
   "metadata": {
    "id": "dae340cb-0583-4e7e-a562-6817ee4d7f6d"
   },
   "source": [
    "# Download the CQADupStack English dataset and export it to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84I4Mepbj0J9",
   "metadata": {
    "id": "84I4Mepbj0J9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://my-project-0004-bucket/download/beir/cqadupstack/english/corpus.jsonl does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/cqadupstack.zip to /var/tmp/tmpqu4siffx/b2d1f3c65bc8aa4cd7b2358d69fcb8df98fcc4f2486e877c.zip\n",
      "Extracting cqadupstack/english/ from /var/tmp/tmpqu4siffx/b2d1f3c65bc8aa4cd7b2358d69fcb8df98fcc4f2486e877c.zip\n",
      "Extracting cqadupstack/english/qrels/ from /var/tmp/tmpqu4siffx/b2d1f3c65bc8aa4cd7b2358d69fcb8df98fcc4f2486e877c.zip\n",
      "Extracting cqadupstack/english/qrels/test.tsv from /var/tmp/tmpqu4siffx/b2d1f3c65bc8aa4cd7b2358d69fcb8df98fcc4f2486e877c.zip\n",
      "Extracting cqadupstack/english/corpus.jsonl from /var/tmp/tmpqu4siffx/b2d1f3c65bc8aa4cd7b2358d69fcb8df98fcc4f2486e877c.zip\n",
      "Extracting cqadupstack/english/queries.jsonl from /var/tmp/tmpqu4siffx/b2d1f3c65bc8aa4cd7b2358d69fcb8df98fcc4f2486e877c.zip\n",
      "Uploading /var/tmp/tmpx14cv328/cqadupstack/english/qrels/train.tsv to gs://my-project-0004-bucket/download/beir/cqadupstack/english/qrels/train.tsv\n",
      "Uploading /var/tmp/tmpx14cv328/cqadupstack/english/qrels/test.tsv to gs://my-project-0004-bucket/download/beir/cqadupstack/english/qrels/test.tsv\n",
      "Uploading /var/tmp/tmpx14cv328/cqadupstack/english/queries.jsonl to gs://my-project-0004-bucket/download/beir/cqadupstack/english/queries.jsonl\n",
      "Uploading /var/tmp/tmpx14cv328/cqadupstack/english/corpus.jsonl to gs://my-project-0004-bucket/download/beir/cqadupstack/english/corpus.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 'gs://my-project-0004-bucket/download/beir/cqadupstack/english/qrels/train.tsv',\n",
       " 'test': 'gs://my-project-0004-bucket/download/beir/cqadupstack/english/qrels/test.tsv',\n",
       " 'queries': 'gs://my-project-0004-bucket/download/beir/cqadupstack/english/queries.jsonl',\n",
       " 'corpus': 'gs://my-project-0004-bucket/download/beir/cqadupstack/english/corpus.jsonl'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the data to GCS and get the paths\n",
    "output_paths = download_to_gcs(DATA_URL, BUCKET_NAME, INPUT_DATA_PREFIX)\n",
    "path_by_type = get_path_by_type(output_paths)\n",
    "\n",
    "OUTPUT_DIR = os.path.join(\n",
    "    os.path.dirname(path_by_type['queries']),\n",
    "    OUTPUT_SUBFOLDER,\n",
    "    )\n",
    "path_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oClNqo8y1d_6",
   "metadata": {
    "id": "oClNqo8y1d_6"
   },
   "source": [
    "# Show some example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0p28kJbVh7iP",
   "metadata": {
    "id": "0p28kJbVh7iP"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19399</th>\n",
       "      <td>Is \"a wide range of features\" singular or plural?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5987</th>\n",
       "      <td>Is there any rule for the placement of space after and before parenthesis?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57488</th>\n",
       "      <td>Word for two people who are the same age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97154</th>\n",
       "      <td>What’s the pronunciation of “ s’ ”?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>What is the correct plural of octopus?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195514</th>\n",
       "      <td>is \"is\" or \"are correct?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88027</th>\n",
       "      <td>What weather! What a pity! - phrases with and without article - why?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53730</th>\n",
       "      <td>\"Aren't I\" vs \"Amn't I\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10705</th>\n",
       "      <td>When do the \"-uple\"s end?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21616</th>\n",
       "      <td>How are \"yes\" and \"no\" formatted in sentences?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              text\n",
       "_id                                                                               \n",
       "19399                            Is \"a wide range of features\" singular or plural?\n",
       "5987    Is there any rule for the placement of space after and before parenthesis?\n",
       "57488                                     Word for two people who are the same age\n",
       "97154                                          What’s the pronunciation of “ s’ ”?\n",
       "270                                         What is the correct plural of octopus?\n",
       "195514                                                    is \"is\" or \"are correct?\n",
       "88027         What weather! What a pity! - phrases with and without article - why?\n",
       "53730                                                      \"Aren't I\" vs \"Amn't I\"\n",
       "10705                                                    When do the \"-uple\"s end?\n",
       "21616                               How are \"yes\" and \"no\" formatted in sentences?"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown Example Queries. { display-mode: \"form\" }\n",
    "number_of_example_queries = 10 # @param {type:\"integer\"}\n",
    "query_df = load_jsonl_from_gcs(path_by_type['queries'], nrows=number_of_example_queries).set_index('_id')\n",
    "query_df[['text']].head(number_of_example_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1BQeQNkX5K0T",
   "metadata": {
    "id": "1BQeQNkX5K0T"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11547</th>\n",
       "      <td>An eponym is one way to eternal (if posthumous) fame. But is there a word meaning an eponym some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11549</th>\n",
       "      <td>We are working on a web project that has a password reset feature. Now the problem is, between \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123851</th>\n",
       "      <td>There are many homonyms in the English language, words that are spelled the same and pronounced ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116661</th>\n",
       "      <td>Some say the following two phrases are equivalent because of Raising (linguistics)! Example 1 &gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102236</th>\n",
       "      <td>Please tell me if the following sentence requires \"have\" or \"has\": &gt; My degree in Cell Biology a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91901</th>\n",
       "      <td>&gt; **Possible Duplicate:**   &gt;  Is the usage of “are” correct when referring to a team/group/band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177507</th>\n",
       "      <td>Let's say we have a group of people who starts an activity together. The name of group can be:  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80798</th>\n",
       "      <td>&gt; **Possible Duplicate:**   &gt;  Are collective nouns always plural, or are certain ones singular?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112990</th>\n",
       "      <td>&gt; 1) Our team of nationally recognized trainers has earned multiple titles…. In the first versio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182056</th>\n",
       "      <td>In the following statement, which one is grammatically correct? &gt; XYZ caterers **is** on to some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       text\n",
       "_id                                                                                                        \n",
       "11547   An eponym is one way to eternal (if posthumous) fame. But is there a word meaning an eponym some...\n",
       "11549   We are working on a web project that has a password reset feature. Now the problem is, between \"...\n",
       "123851  There are many homonyms in the English language, words that are spelled the same and pronounced ...\n",
       "116661  Some say the following two phrases are equivalent because of Raising (linguistics)! Example 1 > ...\n",
       "102236  Please tell me if the following sentence requires \"have\" or \"has\": > My degree in Cell Biology a...\n",
       "91901   > **Possible Duplicate:**   >  Is the usage of “are” correct when referring to a team/group/band...\n",
       "177507  Let's say we have a group of people who starts an activity together. The name of group can be:  ...\n",
       "80798   > **Possible Duplicate:**   >  Are collective nouns always plural, or are certain ones singular?...\n",
       "112990  > 1) Our team of nationally recognized trainers has earned multiple titles…. In the first versio...\n",
       "182056  In the following statement, which one is grammatically correct? > XYZ caterers **is** on to some..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown Example Documents. { display-mode: \"form\" }\n",
    "number_of_example_documents = 10 # @param {type:\"integer\"}\n",
    "corpus_df = load_jsonl_from_gcs(path_by_type['corpus'], nrows=number_of_example_queries).set_index('_id')\n",
    "corpus_df[['text']].head(number_of_example_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "FIXwHAF5OTeP",
   "metadata": {
    "id": "FIXwHAF5OTeP"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100173</td>\n",
       "      <td>147988</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100414</td>\n",
       "      <td>60956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100605</td>\n",
       "      <td>184738</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100664</td>\n",
       "      <td>8892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100664</td>\n",
       "      <td>25375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query-id  corpus-id  score\n",
       "0    100173     147988      1\n",
       "1    100414      60956      1\n",
       "2    100605     184738      1\n",
       "3    100664       8892      1\n",
       "4    100664      25375      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = load_tsv_from_gcs(path_by_type['train'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WZqdLyDzyWNP",
   "metadata": {
    "id": "WZqdLyDzyWNP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
