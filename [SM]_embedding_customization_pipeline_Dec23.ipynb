{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1-de4rAYIuv"
   },
   "source": [
    "# Colab for Creating an Embedding Customization Pipeline\n",
    "\n",
    "Requirements:\n",
    "- kfp>=2.0.0b16\n",
    "- google-cloud-aiplatform>=1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ii_ElkUYAVar",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.26dev20230530+language_models_eval\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PuMHvDVsft0K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 04:36:21.154483: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-28 04:36:21.952205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-28 04:36:21.952307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-28 04:36:22.100817: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-28 04:36:22.395631: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# @title ## Authenticate. { display-mode: \"form\"}\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "# from google.colab import auth as google_auth\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://my-project-0004-bucket/download/beir/cqadupstack/english/qrels/test.tsv\n",
      "gs://my-project-0004-bucket/download/beir/cqadupstack/english/qrels/train.tsv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls 'gs://my-project-0004-bucket/download/beir/cqadupstack/english/qrels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eGa4MSqRocjS"
   },
   "outputs": [],
   "source": [
    "# @title ## Creating Pipeline. { display-mode: \"form\"}\n",
    "# @markdown Creating pipeline job from the pipeline template published to KFP artifact repository. { display-mode: \"form\"}\n",
    "MODEL_NAME = \"text-bison@001\" # @param {type:\"string\"}\n",
    "\n",
    "#gs://my-project-0004-bucket/\n",
    "\n",
    "VERTEX_API_PROJECT = PROJECT_ID = \"my-project-0004-346516\" #'your-project' #@param {\"type\": \"string\"}\n",
    "VERTEX_API_LOCATION =REGION= LOCATION= 'us-central1' #'asia-southeast1' #@param {\"type\": \"string\"}\n",
    "\n",
    "BUCKET_NAME = 'my-project-0004-bucket' # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "# @markdown Update the bucket name to reflect the source of your data. The service account `service-<project_number>@gcp-sa-aiplatform.iam.gserviceaccount.com` must have read and write objects to the GCS buckets. See the documentation for more information about the [Vertex service agents](https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents) and [Cloud Storage permissions](https://cloud.google.com/vertex-ai/docs/general/access-control#storage-roles).\n",
    "QUERIES_PATH = 'gs://my-project-0004-bucket/download/beir/cqadupstack/english/queries.jsonl'  # @param {type:\"string\"}\n",
    "CORPUS_PATH = 'gs://my-project-0004-bucket/download/beir/cqadupstack/english/corpus.jsonl'  # @param {type:\"string\"}\n",
    "TRAIN_LABEL_PATH = 'gs://my-project-0004-bucket/download/beir/cqadupstack/english/qrels/train.tsv'  # @param {type:\"string\"}\n",
    "PIPELINE_ROOT = 'gs://my-project-0004-bucket'  # @param {type:\"string\"}\n",
    "# @markdown The validation label path is optional and if not specified the training data will be randomly split so 20% is used for validation.\n",
    "VALIDATION_LABEL_PATH = ''  # @param {type:\"string\"}\n",
    "# @markdown The test label path is optional and if not specified the training data will be randomly split so 30% is used for testing.\n",
    "TEST_LABEL_PATH = 'gs://my-project-0004-bucket/download/beir/cqadupstack/english/qrels/test.tsv'  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown The following parameters can generally be left at their default setting.\n",
    "BATCH_SIZE = 64  # @param {type:\"integer\"}\n",
    "ITERATIONS = 100  # @param {type:\"integer\"}\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'  # @param {type:\"string\"}\n",
    "MACHINE_TYPE = 'n1-standard-16'  # @param {type:\"string\"}\n",
    "# MACHINE_TYPE = 'a2-highgpu-1g' #\"a2-highgpu-1g\"\n",
    "# ACCELERATOR_TYPE = \"NVIDIA_TESLA_A100\"\n",
    "ACCELERATOR_COUNT = 1\n",
    "\n",
    "PARAMS = {\n",
    "    'project': PROJECT_ID,\n",
    "    'location': LOCATION,\n",
    "    'queries_path': QUERIES_PATH,\n",
    "    'corpus_path': CORPUS_PATH,\n",
    "    'train_label_path': TRAIN_LABEL_PATH,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'iterations': ITERATIONS,\n",
    "    'accelerator_type': ACCELERATOR_TYPE,\n",
    "    'machine_type': MACHINE_TYPE,\n",
    "    # 'accelerator_count' : ACCELERATOR_COUNT,\n",
    "}\n",
    "if VALIDATION_LABEL_PATH:\n",
    "  PARAMS['validation_label_path'] = VALIDATION_LABEL_PATH\n",
    "if TEST_LABEL_PATH:\n",
    "  PARAMS['test_label_path'] = TEST_LABEL_PATH\n",
    "\n",
    "# template_uri = 'https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/custom-embedding-finetuning-pipeline/preview'\n",
    "template_uri = \"https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model/v1.1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZvsF3siQhn1D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when trying to get or create a GCS bucket for the pipeline output artifacts\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py\", line 398, in submit\n",
      "    gcs_utils.create_gcs_bucket_for_pipeline_artifacts_if_it_does_not_exist(\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/google/cloud/aiplatform/utils/gcs_utils.py\", line 235, in create_gcs_bucket_for_pipeline_artifacts_if_it_does_not_exist\n",
      "    pipelines_bucket = storage.Blob.from_string(\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/google/cloud/storage/blob.py\", line 410, in from_string\n",
      "    raise ValueError(\"URI scheme must be gs\")\n",
      "ValueError: URI scheme must be gs\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tune-text-embedding-model-20231128043632?project=255766800726\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/255766800726/locations/us-central1/pipelineJobs/tune-text-embedding-model-20231128043632\n"
     ]
    }
   ],
   "source": [
    "# Run the job. This will launch and then monitor the job until completion.\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name='tune-text-embedding',\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    template_path=template_uri,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    parameter_values=PARAMS,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0nan3ClGIsZw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/255766800726/locations/us-central1/endpoints/2477808826821115904/operations/1192826561150058496\n",
      "Endpoint created. Resource name: projects/255766800726/locations/us-central1/endpoints/2477808826821115904\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/255766800726/locations/us-central1/endpoints/2477808826821115904')\n"
     ]
    }
   ],
   "source": [
    "# @title ## Define the training job and endpoint.\n",
    "# @markdown Define the endpoint id if you'd like to use an existing endpoint. Otherwise leave blank and a new endpoint will be created.\n",
    "# ex: projects/{PROJECT_NUMBER}/locations/{LOCATION}/endpoints/{ENDPOINT_NUMBER}\n",
    "FULL_ENDPOINT_ID = ''  # @param {type:\"string\"}\n",
    "# @markdown Define the job id if you'd like to use an existing job. If you'd like to use the submitted job from above this can be left blank.\n",
    "# ex: projects/{PROJECT_NUMBER}/locations/{LOCATION}/pipelineJobs/{PIPELINE_NAME}\n",
    "FULL_JOB_ID = ''  # @param {type:\"string\"}\n",
    "\n",
    "if FULL_ENDPOINT_ID:\n",
    "  custom_endpoint = aiplatform.Endpoint(\n",
    "      FULL_ENDPOINT_ID, project=PROJECT_ID, location=LOCATION\n",
    "  )\n",
    "else:\n",
    "  custom_endpoint = aiplatform.Endpoint.create(\n",
    "      display_name='tuned_custom_embedding_endpoint',\n",
    "      description='Endpoint for customized embeddings.',\n",
    "      project=PROJECT_ID,\n",
    "      location=LOCATION,\n",
    "  )\n",
    "\n",
    "if FULL_JOB_ID:\n",
    "  job = aiplatform.PipelineJob.get(FULL_JOB_ID, project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "h4cy8Vi0awPK"
   },
   "outputs": [],
   "source": [
    "# @markdown Define helper functions { display-mode: \"form\"}\n",
    "def get_task_by_name(job: aiplatform.PipelineJob, task_name: str):\n",
    "  for task in job.task_details:\n",
    "    if task.task_name == task_name:\n",
    "      return task\n",
    "  raise ValueError(f'Task {task_name} not found')\n",
    "\n",
    "\n",
    "def get_uploaded_model(\n",
    "    job: aiplatform.PipelineJob, task_name: str = 'model-upload'\n",
    ") -> aiplatform.Model:\n",
    "  upload_task = get_task_by_name(job, task_name)\n",
    "  upload_artifact = upload_task.outputs['model'].artifacts[0]\n",
    "  return aiplatform.Model(upload_artifact.metadata['resourceName'])\n",
    "\n",
    "\n",
    "def get_training_output_dir(\n",
    "    job: aiplatform.PipelineJob, task_name: str = 'text-embedding-trainer'\n",
    ") -> str:\n",
    "  trainer_task = get_task_by_name(job, task_name)\n",
    "  output_artifact = trainer_task.outputs['training_output'].artifacts[0]\n",
    "  return output_artifact.uri\n",
    "\n",
    "\n",
    "def get_df_from_jsonl(file_path: str, nrows: int | None = None) -> pd.DataFrame:\n",
    "  with tf.io.gfile.GFile(file_path, 'r') as f:\n",
    "    df = pd.read_json(f, lines=True, nrows=nrows)\n",
    "  return df\n",
    "\n",
    "\n",
    "def get_topk_scores(\n",
    "    query_embedding: pd.DataFrame, corpus_embeddings: pd.DataFrame, k=10\n",
    ") -> pd.DataFrame:\n",
    "  similarity = corpus_embeddings.dot(query_embedding.T)\n",
    "  topk_index = pd.DataFrame(\n",
    "      {c: v.nlargest(n=k).index for c, v in similarity.items()}\n",
    "  )\n",
    "  return topk_index\n",
    "\n",
    "\n",
    "def get_topk_documents(\n",
    "    query_text: list[str],\n",
    "    corpus_text: pd.DataFrame,\n",
    "    corpus_embeddings: pd.DataFrame,\n",
    "    k: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "  response = custom_endpoint.predict(instances=query_text)\n",
    "  query_embedding = np.asarray(response.predictions)\n",
    "  topk = get_topk_scores(query_embedding, corpus_embeddings, k)\n",
    "  return pd.DataFrame.from_dict(\n",
    "      {\n",
    "          query_text[c]: corpus_text.loc[v.values].values.ravel()\n",
    "          for c, v in topk.items()\n",
    "      },\n",
    "      orient='columns',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-XaS9WncMGr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying Model projects/255766800726/locations/us-central1/models/950978601979740160 to Endpoint : projects/255766800726/locations/us-central1/endpoints/2477808826821115904\n",
      "Deploy Endpoint model backing LRO: projects/255766800726/locations/us-central1/endpoints/2477808826821115904/operations/4782195464164343808\n"
     ]
    }
   ],
   "source": [
    "# @title ## Deploy the model to the endpoint\n",
    "INFERENCE_ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'  # @param {type:\"string\"}\n",
    "INFERENCE_ACCELERATOR_COUNT = 1  # @param {type:\"integer\"}\n",
    "INFERENCE_MACHINE_TYPE = 'a2-highgpu-1g'  # @param {type:\"string\"}\n",
    "\n",
    "custom_model = get_uploaded_model(job)\n",
    "custom_endpoint.deploy(\n",
    "    custom_model,\n",
    "    accelerator_type=INFERENCE_ACCELERATOR_TYPE,\n",
    "    accelerator_count=INFERENCE_ACCELERATOR_COUNT,\n",
    "    machine_type=INFERENCE_MACHINE_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Bt6h4AiI77Z"
   },
   "outputs": [],
   "source": [
    "# @title ## Perform examples queries against the training corpus.\n",
    "\n",
    "# Load the corpus embeddings from the training run.\n",
    "training_output_dir = get_training_output_dir(job)\n",
    "corpus_embeddings = get_df_from_jsonl(\n",
    "    tf.io.gfile.join(training_output_dir, 'corpus_custom.jsonl')\n",
    ").set_index('_id', drop=True)\n",
    "corpus_text = get_df_from_jsonl(\n",
    "    tf.io.gfile.join(training_output_dir, 'corpus_text.jsonl')\n",
    ").set_index('_id', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwZtcqLlCTYp"
   },
   "outputs": [],
   "source": [
    "# Define custom queries to see the output.\n",
    "\n",
    "queries = [\n",
    "    '''\"Like something more\" or \"like something better\"''',\n",
    "    '''In what occasion could the word “precious” be taken disapprovingly, or sarcastically?''',\n",
    "    '''Is the phrase \"for free\" correct?''',\n",
    "]\n",
    "output = get_topk_documents(queries, corpus_text, corpus_embeddings, k=10)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 200):\n",
    "  display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJrZ9OT9gtib"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
